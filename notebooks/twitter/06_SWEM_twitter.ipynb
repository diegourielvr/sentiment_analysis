{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWEM (Simple Word Embedding-based Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toma los embeddings y aplica operaciones de Pooling para obtener una representación fija de un documento\n",
    "Se pueden emplear diferentes técnicas para obtener Embeddings y diferentes modelos de clasificación.\n",
    "\n",
    "* SWEM-aver:\n",
    "Hace un promedio de los embeddings\n",
    "\n",
    "* SWEM-max\n",
    "Selecciona el valor máximo en cada dimensión del embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Agregar el directorio raiz al PYTHONPATH\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers.train_swem import train_swem\n",
    "from src.trainers.utils import build_datasets, save_metrics\n",
    "from constants.constants_twitter import TWITTER_DATASET_TRAIN_PATH\n",
    "\n",
    "dataset_train, dataset_test, dataset_val = build_datasets(\n",
    "    TWITTER_DATASET_TRAIN_PATH,\n",
    "    test_size=0.3,\n",
    "    val_size=0.5, # 0.5 de 0.3    \n",
    "    random_state=42\n",
    ")\n",
    "print(dataset_train.shape)\n",
    "print(dataset_test.shape)\n",
    "print(dataset_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocesamiento.nlp_spacy import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"en\")\n",
    "\n",
    "dataset_train_tokenized = {}\n",
    "dataset_val_tokenized = {}\n",
    "dataset_test_tokenized = {}\n",
    "\n",
    "dataset_train_tokenized['tokens'] = tokenizer.tokenize(dataset_train['text'], )\n",
    "dataset_val_tokenized['tokens'] = tokenizer.tokenize(dataset_val['text'])\n",
    "dataset_test_tokenized['tokens'] = tokenizer.tokenize(dataset_test['text'])\n",
    "\n",
    "dataset_train_tokenized['polarity'] = dataset_train['polarity']\n",
    "dataset_val_tokenized['polarity'] = dataset_val['polarity']\n",
    "dataset_test_tokenized['polarity'] = dataset_test['polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar con modelo SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers.utils import SVMModelArgs\n",
    "from constants.constants_twitter import EMBEDDING_W2V_TWITTER_PATH\n",
    "\n",
    "svm_model_args = SVMModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_pooling = [\"aver\", \"max\"]\n",
    "list_kernel = [\"linear\", \"rbf\", 'poly']\n",
    "list_C = np.logspace(-4, -1, 20)\n",
    "print(list_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers.utils import save_model\n",
    "from constants.constants_twitter import SWEM_MODEL_PATH, TWITTER_SWEM_METRICS_PATH\n",
    "\n",
    "for pooling in list_pooling:\n",
    "    for kernel in list_kernel:\n",
    "        for C in list_C:\n",
    "            svm_model_args.kernel = kernel\n",
    "            svm_model_args.C = C\n",
    "            model, metrics = train_swem(\n",
    "                dataset_train_tokenized,\n",
    "                dataset_val_tokenized,\n",
    "                embeddings_path=EMBEDDING_W2V_TWITTER_PATH,\n",
    "                pooling=pooling,\n",
    "                classifier=\"svm\",\n",
    "                model_args=svm_model_args\n",
    "            )\n",
    "            print(f\"SWEM SVM {pooling} {kernel} {C}: {metrics['accuracy']}\")\n",
    "            # Guardar Pipeline\n",
    "            save_metrics(metrics, TWITTER_SWEM_METRICS_PATH)\n",
    "            if metrics['accuracy'] > best_accuracy:\n",
    "                best_accuracy = metrics['accuracy']\n",
    "                save_model(model, SWEM_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar con modelo LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers.utils import LRModelArgs\n",
    "\n",
    "lr_model_args = LRModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_pooling = [\"aver\", \"max\"]\n",
    "list_solver_l1 = [\"saga\"]\n",
    "list_solver_l2 = [\"lbfgs\", \"saga\"]\n",
    "list_C = np.logspace(-4, -1, 20)\n",
    "print(list_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.constants_twitter import SWEM_MODEL_PATH, TWITTER_SWEM_METRICS_PATH\n",
    "from src.trainers.utils import save_model\n",
    "\n",
    "for pooling in list_pooling:\n",
    "    for solver in list_solver_l2:\n",
    "        for C in list_C:\n",
    "            lr_model_args.penalty=\"l2\" \n",
    "            lr_model_args.C = C\n",
    "            lr_model_args.max_iter = 300\n",
    "            lr_model_args.solver = solver\n",
    "            \n",
    "            model, metrics = train_swem(\n",
    "                dataset_train_tokenized,\n",
    "                dataset_val_tokenized,\n",
    "                embeddings_path=EMBEDDING_W2V_TWITTER_PATH,\n",
    "                pooling=pooling,\n",
    "                classifier=\"lr\",\n",
    "                model_args=lr_model_args\n",
    "            )\n",
    "            print(f\"SWEM LR l2 {pooling} {solver} {C}: {metrics['accuracy']}\")\n",
    "            save_metrics(metrics, TWITTER_SWEM_METRICS_PATH)\n",
    "            if metrics['accuracy'] > best_accuracy:\n",
    "                best_accuracy = metrics['accuracy']\n",
    "                save_model(model, SWEM_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.constants_twitter import SWEM_MODEL_PATH, TWITTER_SWEM_METRICS_PATH\n",
    "from src.trainers.utils import save_model\n",
    "\n",
    "for pooling in list_pooling:\n",
    "    for solver in list_solver_l1:\n",
    "        for C in list_C:\n",
    "            lr_model_args.penalty=\"l1\" \n",
    "            lr_model_args.C = C\n",
    "            lr_model_args.max_iter = 300\n",
    "            lr_model_args.solver = solver\n",
    "            \n",
    "            model, metrics = train_swem(\n",
    "                dataset_train_tokenized,\n",
    "                dataset_val_tokenized,\n",
    "                embeddings_path=EMBEDDING_W2V_TWITTER_PATH,\n",
    "                pooling=pooling,\n",
    "                classifier=\"lr\",\n",
    "                model_args=lr_model_args\n",
    "            )\n",
    "            print(f\"SWEM LR l1 {pooling} {solver} {C}: {metrics['accuracy']}\")\n",
    "            save_metrics(metrics, TWITTER_SWEM_METRICS_PATH)\n",
    "            if metrics['accuracy'] > best_accuracy:\n",
    "                best_accuracy = metrics['accuracy']\n",
    "                save_model(model, SWEM_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con mayor accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar los hiperparámetros que generan mayor accuracy\n",
    "df_metrics = pd.read_csv(TWITTER_SWEM_METRICS_PATH)\n",
    "\n",
    "best_acc = df_metrics.loc[df_metrics['accuracy'].idxmax()]\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers.train_swem import evaluate_model\n",
    "from src.trainers.utils import load_model\n",
    "\n",
    "# Evaluar modelo con datos de prueba\n",
    "model = load_model(SWEM_MODEL_PATH)\n",
    "metrics = evaluate_model(\n",
    "    model, \n",
    "    dataset_test_tokenized,\n",
    "    \"Simple Word Embedding-based Model\",\n",
    "    EMBEDDING_W2V_TWITTER_PATH,\n",
    "    best_acc['pooling'],\n",
    "    )\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
